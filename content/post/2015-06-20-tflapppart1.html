---
title: Web-scraping and cleaning data with R
date: '2015-06-20'
slug: getting-started-with-shiny
categories:
  - R
  - tidyverse
tags: []  
---



<p>In my first post I introduced a simple <a href="http://shiny.rstudio.com/">shiny</a> app that I’ve been putting together in my free time over the last few weeks. This is the first of the promised posts about my app - covering how I went about getting and cleaning the data for the app. The full code is on my <a href="https://github.com/Jim89/oyster">GitHub</a> page so this post will just be a few highlights.</p>
<p>I’ve broken this down in to two sections. The first is a brief overview of what TfL provide in their weekly <em>csv</em> files and the second is a (more interesting) look at how I solved some problems with web-scraping in R and the fantastic <a href="http://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/">rvest</a> package. Hopefully I can highlight some of the benefits of rvest and this might serve as a simple web-scraping in R example.</p>
<div id="tfl-oyster-card-data-wrangling-in-r" class="section level2">
<h2>Tfl Oyster Card Data Wrangling in R</h2>
<p>The data provided by the <a href="https://tfl.gov.uk/fares-and-payments/oyster">TfL Oyster</a> system are quite messy:</p>
<ul>
<li>only the journey start date is given (meaning some simple manipulation is required to get the date for journeys ending after midnight),</li>
<li>both/all stations in a journey are in one text field, and must be parsed out.</li>
<li>dates/times are given as text, not as date/time or datetime fields</li>
<li>bus journeys are simply listed as “bus journey on route [bus route]”.</li>
</ul>
<p>You can see the full extent of the various manipulations I had to make in the code file on GitHub <a href="https://github.com/Jim89/oyster/blob/master/gettingAndCleaning/01_oysterData.R">here</a>. Let’s just say that <a href="http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html">dplyr</a>, <a href="http://blog.rstudio.org/2015/05/05/stringr-1-0-0/">stringr</a> and <a href="http://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html">lubridate</a> were all fantastic tools for getting everything sorted in to a nice, clean and tidy table.</p>
<p>Looking at the data as provided by TfL, I knew that I could do some simple analysis of journeys, times spent on transport etc (more on that in a later post). But I wanted to do something cool with the <a href="https://rstudio.github.io/leaflet/">leaflet</a> package and put my data on a map. So, as the raw data don’t come with geographic locations (beyond station names), I’d have to get that data myself.</p>
</div>
<div id="web-scraping-with-r" class="section level2">
<h2>Web-scraping with R</h2>
<p>Wikipedia already contains a clean table of <a href="http://en.wikipedia.org/wiki/List_of_London_railway_stations">London railway station co-ordinates</a> so I wrote a quick function to do a look up to grab the table. However it doesn’t have the underground stations - just mainline rail services - so I had to do some more digging.</p>
</div>
<div id="station-locations" class="section level2">
<h2>Station locations</h2>
<p>Looking at the page source for a handful of underground stations I spotted that they all contained the co-ordinates as part of the page. A colleague suggested trying some web-scraping to get at them on wikipedia. I hadn’t done this with R before but thought it sounded fun so I turned to the then-newly-released <a href="http://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/">rvest</a> package.</p>
<p>(If I’d done some more googling I would have found the <code>geocode()</code> function from ggplot2 and saved myself the trouble. This was an interesting problem to solve, though, and gave me some good experience with web-scraping in R.)</p>
<p>My first step was to pick a starting station page, and grab all the links from it (I went with Baker Street). The function is very simple. Given an input url it:</p>
<ul>
<li>scans the web page,</li>
<li>finds all the links,</li>
<li>grabs the link url (held in the html ‘href’ attribute), and</li>
<li>returns a character vector of all the links.</li>
</ul>
<p>As I was only looking for links to wikipedia pages that were for London underground stations I used a couple of simple ‘grep’ steps to narrow down my search a bit.</p>
<div id="extracting-page-links-from-wikipedia" class="section level4">
<h4>Extracting page links from Wikipedia</h4>
<pre class="r"><code>ExtractLinks &lt;-
function(url = &quot;http://en.wikipedia.org/wiki/Baker_Street_tube_station&quot;) {
  # takes a URL and extracts all links from it
  url %&gt;%
    html() %&gt;%
    html_nodes(&quot;a&quot;) %&gt;%
    html_attr(&quot;href&quot;) %&gt;%
    grep(&quot;wiki&quot;, ., value = T) %&gt;%
    grep(&quot;station&quot;, ., value = T) %&gt;%
    paste0(&quot;http://en.wikipedia.org&quot;,.)
}</code></pre>
<p>After that I needed a way to get at the coordinates data I’d seen on each page. Some playing with the <a href="http://selectorgadget.com/">selectorgadget</a> tool gave me the html node on each page that contained the coordinates data. A quick rvest function got me a messy set of coordinates data.</p>
</div>
<div id="scraping-data-from-wikipedia-pages" class="section level4">
<h4>Scraping data from wikipedia pages</h4>
<pre class="r"><code>ExtractGeo &lt;- function(url) {
  # takes a URL and uses the CSS selector provided by SelectorGadget widget to
  # extract the dirty coordinates data from it
  url %&gt;%
    html() %&gt;%
    html_nodes(&quot;span span span span span span.geo&quot;) %&gt;%
    html_text()
}</code></pre>
<p>However I also needed the station name to link back to my TfL data, so I put together another function to get that from the wiki page, too:</p>
<pre class="r"><code>ExtractTitle &lt;- function(url) {
  # takes a URL and uses the CSS selector provided by SelectorGadget widget to
  # extract the first header, which in this case is the wikipedia page title
  url %&gt;%
    html() %&gt;%
    html_nodes(&quot;div h1&quot;) %&gt;%
    html_text()
}</code></pre>
<p>Once I’d created these functions I proceeded to:</p>
<ol style="list-style-type: decimal">
<li>Run the function to get all the links from a station page - this gave me all the TfL underground station pages on wikipedia.</li>
<li>Loop over all of these links to get the station coordinates and name using the second two functions.</li>
</ol>
<p>So far, so-straight-forwards. However the data-scrape was still very messy and not fit for my purposes. The full code is <a href="https://github.com/Jim89/oyster/blob/master/gettingAndCleaning/02_stationsData.R">here</a> but to put it simply I had to:</p>
<ul>
<li>use a lot of <code>sapply()</code> and functions from the stringr package (mainly <code>str_extract()</code>),</li>
<li>clean up some guff I’d scraped as part of my wide-shot approach (mainly page errors and elements I didn’t need),</li>
<li>clean up some individual station names that I knew wouldn’t match to the data from TfL, and</li>
<li>run my scraping functions one-by-one for a few stations that I’d missed initially.</li>
</ul>
</div>
</div>
<div id="wrapping-up-and-thoughts" class="section level2">
<h2>Wrapping up and thoughts</h2>
<p>By the end of all this I had my hands on a clean and tidy data set containing station name, longitude and lattitude - perfect for plotting my data on to a map. A quick use of dplyr’s <code>left_join()</code> and I had everything I needed to get going.</p>
<p>rvest was incredibly easy to use once I’d figured out the various functions and gave me exactly what I was looking for. I can definitely see myself turning to it again in future when I’m dealing with web-based data. Definitely easier to navigate html/XML with it than the (older but still useful) XML package. dplyr, lubridate and stringr (yet) again proved their fantastic-ness. Hadley Wickham (and anyone else who contributes but gets less recognition) deserve a medal.</p>
<p>In my next post I’ll talk about how I got on with ggplot2 and putting my data on the map before I pen a third on the app itself.</p>
</div>
